\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\title{Метод мультистилевого рендеринга изображений}

\author{
  Загатин Данилл Ильич \\
  МГУ им. М.В. Ломоносова \\
  Факультет ВМК, кафедра Математических методов прогнозирования \\
  Москва, Россия \\
  \texttt{Почта тут} \\
  \And
  Научный руководитель: к.ф.-м.н., доцент Китов Виктор Владимирович \\
  МГУ им. М.В. Ломоносова
}
\date{}

\renewcommand{\shorttitle}{Multi-style Rendering Method}


\hypersetup{
  pdftitle={Метод мультистилевого рендеринга изображений},
  pdfsubject={cs.CV},
  pdfauthor={Загатин Данилл Ильич, Китов Виктор Владимирович},
  pdfkeywords={перенос стиля, произвольная стилизация, стилевой эмбеддинг, AdaIN, FiLM, patch-based},
}

\begin{document}
\maketitle

\begin{abstract}
В работе рассматривается задача произвольной стилизации изображений с использованием сверточных нейронных сетей (CNN). Большинство моделей обучаются для конкретного стиля и успешно передают палитры и текстуры, однако часто не воспроизводят уникальные стилистические особенности, склоняясь к усреднённым мазкам и цветам. На основе методов Гатиса, Джонсона и Гиаси исследуются модификации генератора с кодировщиком стиля и различными способами внедрения эмбеддингов: конкатенацией, 1×1-инъекцией, FiLM-блоками и патчевыми методами. Работа выявляет архитектурные ограничения и предлагает улучшения: усиление тренировки кодировщика, раздельное внедрение компонентов стиля и использование альтернативных функций потерь. Разработана мультистилевая лёгкая архитектура с высоким качеством стилизации и гибкостью применения к разнообразным стилям.
\end{abstract}

\keywords{перенос стиля \and свёрточные нейронные сети \and мультистилевая стилизация \and стилевой эмбеддинг \and FiLM}

\section{Введение}
Перенос художественного стиля (Neural Style Transfer, NST) позволяет преобразовывать изображение, сохраняя его семантическое содержимое и воспроизводя художественные особенности другого изображения. Практическая ценность NST растёт в цифровом искусстве, дизайне, AR/VR и мобильных приложениях, где важны интерактивность, персонализация и низкая стоимость генерации контента.

Классические оптимизационные подходы обеспечивают высокое качество, но требуют длительной итеративной подгонки под каждую новую пару изображений. Быстрые генеративные сети позволяют работать в реальном времени, однако в базовом варианте обучаются под один конкретный стиль, что ограничивает масштабирование. Универсальные методы произвольной стилизации снимают это ограничение, но часто теряют характерные структурные элементы стиля или страдают от артефактов и нестабильного обучения.

В данной работе рассматривается мультистилевая схема рендеринга, где стилевое изображение кодируется в компактный эмбеддинг и внедряется в генератор различными механизмами: прямой конкатенацией к признаковым картам, 1{\texttimes}1-инъекцией (learnable projection), FiLM-модуляцией, а также через патчевое сопоставление признаков. Такой дизайн объединяет скорость feed-forward архитектур с гибкостью произвольной стилизации.

Наш вклад — структурированное сравнение способов внедрения стилевого эмбеддинга, практические рекомендации по устойчивому обучению кодировщика стиля и анализ причин деградации качества (усреднение палитры и мазков, артефакты от локальных сопоставлений). Мы также обсуждаем улучшения: усиление тренировки кодировщика контрастными целями, раздельное внедрение палитры и текстур и альтернативные функции потерь для сохранения локальных деталей.

\medskip
\section{Обзор литературы.}
Базовый подход \citet{gatys2015} формулирует NST как оптимизацию изображения под перцептивные потери на активациях VGG: контент фиксируется через промежуточные признаки, стиль — через матрицы Грама корреляций признаков. Это даёт качественную стилизацию, но непрактично для интерактивных сценариев.

Переход к генераторам реального времени осуществлён в \citet{johnson2016}: сеть-преобразователь с residual-блоками обучается по перцептивным лоссам и стилизует за один прямой проход. Существенную роль в стабильности генерации сыграла Instance Normalization \citep{ulyanov2016instnorm}. Однако такие модели обычно требуют отдельного обучения под каждый стиль.

Поддержка произвольных стилей достигается статистическими методами выравнивания признаков: AdaIN выравнивает среднее и дисперсию каналов \citep{huang2017adain}, WCT применяет whitening\&coloring трансформы \citep{li2017wct}. Работа \citet{ghiasi2017magentanet} использует стиль-кодировщик для предсказания параметров нормализации слоёв генератора, внедряя стиль непосредственно в механизм нормализации. Эти подходы быстры и гибки, но при сильной вариативности стилей могут терять локальные текстуры и характерные мазки.

Локально-структурные (патчевые) методы сопоставляют фрагменты признаков контента и стиля \citep{chen2016fastpatch} или комбинируют CNN с MRF \citep{li2016cnnmrf}, что помогает передавать текстуры и повторяющиеся мотивы, но повышает требования к памяти и склонно к артефактам. Современные модели внимания усиливают согласование стиля и контента (SANet \citep{li2019sanet}, AdaAttN \citep{he2019adaattn}), а трансформерные решения (StyTR$^2$ \citep{xia2022stytr}) улучшают глобальные зависимости ценой усложнения архитектуры и обучения.

Наконец, условные модули (например, FiLM \citep{perez2018film}) линейно модулируют признаки генератора параметрами, зависящими от условия (стиля), и предоставляют простой общий механизм внедрения стилевой информации; их практическая эффективность чувствительна к качеству стилевого эмбеддинга и балансу лоссов.

\section{Постановка задачи}

Задача мультистилевой стилизации изображений формулируется как построение отображения между пространством контентных изображений и пространством стилевых изображений. Пусть  
$X_c \subset \mathbb{R}^{H \times W \times 3}$ — множество контентных изображений,  
$X_s \subset \mathbb{R}^{H \times W \times 3}$ — множество стилевых изображений.  
Каждая пара $(x_c, x_s)$ порождает целевое изображение $y = f^*(x_c, x_s)$, сохраняющее семантическое содержание контента и визуальные особенности стиля.

\subsection*{Алгебраическая структура данных}

Контентное изображение $x_c$ можно рассматривать как тензор признаков, описывающий пространственную структуру сцены — расположение объектов, контуры и композицию.  
Стилевое изображение $x_s$, напротив, несёт информацию о глобальных художественных характеристиках: цветовой палитре, контрасте, фактуре мазков и текстуре.  

В скрытом пространстве признаков $\Phi(\cdot)$, извлечённом свёрточной сетью (например, VGG), стиль кодируется в компактное векторное представление — \textbf{стилевой эмбеддинг}:
$$
z_s = e_\psi(x_s) \in \mathbb{R}^d,
$$
где $e_\psi$ — кодировщик стиля.  
Вектор $z_s$ агрегирует глобальные статистики активаций и отражает основные визуальные свойства стиля — тональность, насыщенность, характер штрихов и степень контрастности.  

Таким образом, пара $(x_c, x_s)$ задаёт комбинацию двух типов признаков: структурных (контентных) и художественных (стилевых).  
Цель модели — построить отображение $f(x_c, x_s)$, которое сохраняет геометрию контента $x_c$ и интегрирует в него глобальные стилевые свойства, закодированные в векторе $z_s$.

\subsection*{Отображение $f : (x_c, x_s) \mapsto y$}

Искомое отображение реализуется нейросетевой моделью вида:
$$
f_\theta(x_c, x_s) = G_\theta(x_c, e_\psi(x_s)),
$$
где $G_\theta$ — генератор, выполняющий преобразование контентного изображения с учётом стилевого эмбеддинга $z_s = e_\psi(x_s)$.  

В зависимости от архитектуры, внедрение стилевого вектора $z_s$ в генератор может осуществляться несколькими способами:
\begin{enumerate}
    \item \textbf{Прямая конкатенация} — добавление эмбеддинга к признаковым картам по каналам;
    \item \textbf{Инъекция через обучаемую $1 \times 1$ свёртку} — линейное отображение стилевого вектора в пространстве признаков;
    \item \textbf{FiLM-модуляция} — параметрическое масштабирование и смещение карты признаков по формулам $\gamma(z_s) \cdot x + \beta(z_s)$;
    \item \textbf{Патчевое сопоставление} — замена локальных участков признаков контента наиболее близкими фрагментами признаков стиля.
\end{enumerate}

Эти подходы различаются степенью локальности передачи стилевой информации и устойчивостью к вариациям стиля.

\subsection*{Внешний критерий качества}

Качество стилизованного изображения оценивается по комбинации перцептивных потерь, вычисляемых на предобученной сети $\Phi$:
$$
\mathcal{L}_{total} = \lambda_c \mathcal{L}_{content} + \lambda_s \mathcal{L}_{style} + \lambda_{TV}\mathcal{L}_{TV},
$$
где
$$
\mathcal{L}_{content} = \| \Phi_l(y) - \Phi_l(x_c) \|_2^2, \qquad
\mathcal{L}_{style} = \sum_{l} \| G(\Phi_l(y)) - G(\Phi_l(x_s)) \|_2^2,
$$
а $\mathcal{L}_{TV}$ — регуляризатор общей вариации, способствующий сглаживанию артефактов.  
Матрица Грама $G(\Phi_l(\cdot))$ отражает корреляции признаков на слое $l$ и используется как статистическое описание стиля.
\section{Описание метода}

Предлагаемый метод направлен на решение задачи произвольной стилизации изображений с использованием единой архитектуры, способной обрабатывать множество стилей без переобучения.  
В основе лежит генеративная сеть, которая принимает на вход контентное изображение $x_c$ и векторное представление стиля $z_s = e_\psi(x_s)$, формируемое кодировщиком стиля.  
Модель состоит из двух ключевых компонентов: \textbf{кодировщика стиля} и \textbf{генератора}, интегрирующего стилевую информацию различными способами.

\subsection*{Кодировщик стиля}

Кодировщик стиля $e_\psi$ предназначен для извлечения информативного эмбеддинга из стилевого изображения $x_s$.  
В качестве основы используется модифицированная архитектура \textit{Inception-v3}, усечённая до блока \texttt{Mixed\_6e}.  
Эта часть сети эффективно извлекает визуальные признаки высокого уровня, обученные на датасете ImageNet.  

После сверточных слоёв применяется глобальный усредняющий пуллинг (GAP), формирующий вектор фиксированной длины. Далее используется полносвязный слой, отображающий признаки в пространство стилевых эмбеддингов размерности $d$.  
Итоговый вектор нормализуется по $L_2$-норме и масштабируется обучаемым коэффициентом $\alpha$:
$$
z_s = \alpha \cdot \frac{h}{\|h\|_2}, \qquad h = \mathrm{FC}(\mathrm{GAP}(\mathrm{Inception}(x_s))).
$$

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.85\linewidth]{style_enc.png}
    \caption{Схема кодировщика стиля: сверточная часть на основе Inception-v3 для извлечения признаков, глобальный усредняющий пуллинг и линейное отображение в пространство стилевых эмбеддингов.}
    \label{fig:style_encoder}
\end{figure}

Для повышения устойчивости кодировщика он предварительно дообучается на задаче классификации художественных стилей.  
В качестве обучающих данных используется выборка из \textit{WikiArt}, содержащая изображения 25 направлений живописи.  
После дообучения кодировщик способен формировать эмбеддинги, группирующиеся в соответствии со стилевыми классами, что подтверждается визуализацией t-SNE.  
Такой подход обеспечивает информативное и устойчивое представление стиля, пригодное для генерации.

\subsection*{Архитектура генератора}

Генератор $G_\theta$ построен на основе архитектуры Johnson et al.  
Он имеет симметричную структуру, включающую слои понижения и повышения разрешения, а также серию residual-блоков в центральной части (bottleneck).  

Архитектура включает три этапа:
\begin{enumerate}
    \item \textbf{Downsampling:} три последовательных свёрточных слоя (ядра $9 \times 9$ и $3 \times 3$, два из них со stride = 2), каждый сопровождается Instance Normalization и ReLU;
    \item \textbf{Bottleneck:} несколько residual-блоков, обрабатывающих карту признаков пониженного разрешения;
    \item \textbf{Upsampling:} два блока Upsample+Conv, восстанавливающих исходное разрешение изображения, с InstanceNorm и ReLU-активацией;
\end{enumerate}
Финальный слой свёртки с ядром $9 \times 9$ формирует трёхканальное RGB-изображение.

\subsection*{Механизмы внедрения стиля}

Ключевое отличие метода заключается в исследовании различных способов внедрения стилевого эмбеддинга $z_s$ в генератор.  
Были реализованы и сравнены несколько механизмов интеграции:

\begin{enumerate}
    \item \textbf{Конкатенация (ResidualBlockConcat):}  
    В каждом residual-блоке эмбеддинг $z_s$ расширяется по пространственным координатам и конкатенируется с признаковой картой перед каждой свёрткой.  
    Это обеспечивает прямую подачу стилевой информации на каждом уровне обработки.

    \item \textbf{Инъекция через $1 \times 1$-свёртку (ResidualBlockInject):}  
    Стилевой вектор преобразуется в карту признаков с помощью обучаемого слоя $1 \times 1$ Conv, результат которого добавляется к промежуточным признакам внутри residual-блока:
    $$
    F' = F + \mathrm{Conv}_{1\times1}(z_s).
    $$
    Такой подход позволяет линейно внедрять стиль и сохранять пространственную структуру контента.

    \item \textbf{FiLM-модуляция (Feature-wise Linear Modulation):}  
    Эмбеддинг стиля подаётся в два линейных слоя, генерирующих параметры $\gamma(z_s)$ и $\beta(z_s)$, которые затем применяются к признаковой карте:
    $$
    F' = \gamma(z_s) \cdot F + \beta(z_s).
    $$
    Этот метод обеспечивает гибкую настройку интенсивности стиля и является дифференцируемым аналогом адаптивной нормализации (AdaIN).

    \item \textbf{Патчевое сопоставление (Patch-based injection):}  
    Вместо вектора используется карта признаков стиля, извлечённая из кодировщика.  
    Для каждого локального патча карты контента подбирается наиболее похожий патч стиля по косинусному сходству.  
    Обновлённая карта признаков объединяет локальные стилевые паттерны, улучшая передачу текстур:
    $$
    \Phi'(x_c) = \mathrm{PatchMatch}(\Phi(x_c), \Phi(x_s)).
    $$
\end{enumerate}

Эти подходы различаются по локальности внедрения, устойчивости к артефактам и выразительности стиля.  
Практические эксперименты показывают, что методы инъекции через $1 \times 1$-свёртки и FiLM-модуляции обеспечивают более плавную передачу стиля, тогда как патчевые методы улучшают локальные текстуры ценой увеличения вычислительных затрат.

\subsection*{Итоговая архитектура}

В совокупности модель представляет собой систему:
$$
y = G_\theta(x_c, e_\psi(x_s)),
$$
где $e_\psi$ формирует стилевой эмбеддинг, а $G_\theta$ преобразует изображение в стилизованный вид.  
Предложенный метод объединяет скорость feed-forward архитектур с гибкостью произвольной стилизации, обеспечивая возможность применения к множеству стилей в единой сети.


\section{Эксперименты (В процессе)}

\subsection*{Описание данных}

Для проведения экспериментов использовались два набора изображений: контентные и стилевые.  
Контентные изображения взяты из датасета \textit{MS COCO} и представляют собой разнообразные сцены, содержащие объекты, людей и природные элементы.  
Стилевые изображения собраны на основе коллекции \textit{WikiArt}, включающей 50 направлений живописи — от импрессионизма и кубизма до постмодернизма.  

Из каждого стилевого изображения вырезались случайные фрагменты двух размеров: $128 \times 128$ и $256 \times 256$.  
Такая процедура позволяет лучше передавать фактурные особенности мазков, текстуры и цветовые закономерности.  
В общей сложности использовано 10\,000 контентных изображений и 200 стилевых фрагментов.

Примеры изображений приведены на рис.~\ref{fig:content_style_examples}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{content_style_examples.png}
    \caption{Примеры используемых изображений: (слева) контентные изображения из MS~COCO, (справа) стилевые изображения из WikiArt.}
    \label{fig:content_style_examples}
\end{figure}

\subsection*{Схема обучения}

Обучение модели включает совместную оптимизацию параметров генератора $G_\theta$ и кодировщика стиля $e_\psi$.  
На каждой итерации выбирается пара $(x_c, x_s)$, где $x_c$ — контентное изображение, а $x_s$ — случайно выбранный стиль.  
Генератор формирует стилизованное изображение $y = G_\theta(x_c, e_\psi(x_s))$, после чего вычисляются потери по слоям предобученной сети VGG.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.95\linewidth]{train_scheme.png}
%     \caption{Схема обучения модели: кодировщик стиля $e_\psi$ извлекает вектор $z_s$, генератор $G_\theta$ выполняет преобразование $x_c \rightarrow y$, далее вычисляются контентные и стилевые потери на признаках VGG.}
%     \label{fig:train_scheme}
% \end{figure}

Используемая функция потерь имеет вид:
$$
\mathcal{L}_{total} = 
\lambda_c \mathcal{L}_{content}(y, x_c)
+ \lambda_s \mathcal{L}_{style}(y, x_s)
+ \lambda_{TV} \mathcal{L}_{TV}(y),
$$
где весовые коэффициенты выбирались эмпирически:
$\lambda_c = 1.0$, $\lambda_s = 5.0$, $\lambda_{TV} = 10^{-5}$.

\subsection*{Параметры обучения}

Модель обучалась с использованием оптимизатора \textit{Adam} с начальными параметрами:
$$
\text{learning rate } = 5 \times 10^{-4}, \quad \beta_1 = 0.9, \quad \beta_2 = 0.999.
$$
Размер батча составлял 32.  
Обучение проводилось на GPU \textit{NVIDIA Tesla P100} в течение 80 эпох.  
В качестве входных изображений использовались фрагменты размером $256 \times 256$.  

Кодировщик стиля и генератор обучались в две фазы:
\begin{enumerate}
    \item Совместное обучение всех параметров $(\theta, \psi)$;
    \item Заморозка генератора и дополнительное дообучение кодировщика стиля для стабилизации эмбеддингов.
\end{enumerate}

\subsection*{Методика оценки}

Качество стилизации оценивалось по трём группам метрик:
\begin{enumerate}
    \item \textbf{Контентная сохранность} — среднеквадратичное отличие признаков контента:  
    $$
    \mathcal{L}_{content} = \| \Phi_l(y) - \Phi_l(x_c) \|_2^2;
    $$
    \item \textbf{Стилизация} — отклонение матриц Грама между выходным и стилевым изображением:  
    $$
    \mathcal{L}_{style} = \sum_l \| G(\Phi_l(y)) - G(\Phi_l(x_s)) \|_2^2;
    $$
    \item \textbf{ArtFID (Artistic Fréchet Inception Distance)} — основная метрика, измеряющая расстояние между распределениями признаков стилизованных и эталонных стилевых изображений:
    $$
    \text{ArtFID} = \| \mu_y - \mu_s \|_2^2 + \mathrm{Tr}\left( \Sigma_y + \Sigma_s - 2(\Sigma_y \Sigma_s)^{1/2} \right),
    $$
    где $(\mu_y, \Sigma_y)$ и $(\mu_s, \Sigma_s)$ — параметры многомерных гауссовых аппроксимаций распределений признаков изображений $y$ и $x_s$ соответственно, вычисленные в пространстве активаций Inception-сети.  
    Низкое значение ArtFID соответствует высокой визуальной схожести с художественным стилем.
\end{enumerate}



\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
